<!DOCTYPE html> <html> <head> <meta charset="UTF-8"> <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> <meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"> <title>Derrek Chow</title> <meta name="description" content=""> <link rel="shortcut icon" type="image/png" href="/assets/favicon.png"> <link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" /> <link rel="stylesheet" href="/assets/css/raster2.css"> <link rel="stylesheet" href="/assets/css/styles.css"> </head> <body> <main> <r-grid columns=9> <r-cell span=8 style="max-width:880px; margin: auto" id=""> <a href="/#" onClick="if (history.length > 1) { event.preventDefault(); history.back(); }" class="subtitle" >⟵ back to </a><br/><br/><br/> <div class="project"> <article> <r-grid columns=5 class="blocks" columns-s=1> <r-cell span=3> <img src="https://user-images.githubusercontent.com/17170744/160160207-aba67170-8d88-46b0-99c2-861a6babe5e5.png" /> </r-cell> <r-cell span=2> <h1>Iteration 2</h1> <div class="subtitle"> <label>CanHap501, Winter 2022</label><br/><br/> <label>Derrek Chow, Team Hapstrument</label> <br/><br/> </div> <!-- --> My main focus for Iteration 2 was haptic rendering for the right Haply (volume) and building the overall experience with both Haplys. Based on our Iteration 1, our team decided to continue with the strategy of working on each Haply separately, which each member developing features specific to a single Haply. My role was similar to Iteration 1, I worked on controlling volume with the right Haply. The second team member continued working on pitch selection interface for the left Haply, we decided to explore a circular model as opposed to our existing piano interface. The third member would work on volume with the right Haply as well, but using a different approach to me. Since the previous iteration integrated volume control, we would now explore the haptic experience of this control. I explored rendering the sensation of plucking a string to control volume, while the third member explored the a bowing sensation. As in Iteration 1, I took on the role of combining the our individual parts into one (combining both Haplys).<br/><br/>My motivation for this iteration was to 1) create an intuitive and haptic method to control volume and 2) setup our system so that it would be easy to work on separately and then later combine. My approach for the first was to setup a physical string apparatus as a reference recreate the feeling of plucking it on the Haply through small iterations. This approach was inspired to do this by Lab 1 and 2. For the second motivation, my approach was to restructure the code to be modular and loosely coupled. I used an OO strategy with a <i>Haply</i> class responsible for communicating to the Haply. With the constructor of this class, the user passes in a <i>Model</i> instance. I had each member create their own model child class that would inherit from this <i>Model</i> class. That way, it was easy to swap and add different models to the Haply. I setup the code so it would be easy to test with either one or both Haplys. The outcome of this structure was faster development because of less debugging time, it was easy to merge our code together in the end and test everything with minimal changes. <br/><br/>The commented code on the left shows the Pure Data file from the previous iteration. I simplified the code and added comments. We decided to focus more on the Haply side of development so I kept the sound rendering minimal. We will develop this sound file in the next iteration through the addition of timbre. <br/><br/><h3><a href="https://git.uwaterloo.ca/d28chow/canhap501/-/blob/master/iteration_2" target="_blank">Link to the code</a></h3> </r-cell> <hr/> <r-cell span=3> <video src="https://user-images.githubusercontent.com/17170744/160159145-ca6f06ba-6389-406f-acf2-d60d8445b441.mp4" width="100%" controls controlslist="nodownload noremoteplayback"></video> </r-cell> <r-cell span=2> <h2>Haptic Rendering&#58; Volume Control</h2>The left video shows haptic force with volume control for the right Haply. As the user pulls downwards, the volume increases and the Haply provides resistance upwards. This produces a pulling sensation, to me it felt like the metaphor of opening a valve under tension to release air. I based my force code off the calculations of a spring and the volume as a function of its displacement from no tension or resting position (0 force). <span></span> </r-cell> <r-cell span=3> <video src="https://user-images.githubusercontent.com/17170744/160159133-bad469f7-211e-4cee-b280-fef8967bf01b.mp4" width="100%" controls controlslist="nodownload noremoteplayback"></video> </r-cell> <r-cell span=2> The volume control was originally along the x direction but I switched it to the y because I found pulling downwards to feel more natural than moving left or right. I found I was only able to discover this when combining the Haplys together and evaluating the playing experience holistically. The left video shows both Haplys being used, the left with pitch selection on a piano and the right my code controlling volume. <span></span> </r-cell> <hr/> <r-cell span=3> <video src="https://user-images.githubusercontent.com/17170744/160159157-74c547e2-a5ce-4c2b-8524-547d82d1f47a.mp4" width="100%" controls controlslist="nodownload noremoteplayback"></video> </r-cell> <r-cell span=2> <h2>Haptic Rendering&#58; String</h2>The left video shows the right Haply plucking producing the feeling of plucking a string, through a visual and haptics. I explored having the string horizontal instead of vertical like with previous pulling rendering example, but felt vertical to be more natural. This taught me this layout depends on the type of haptic experience, a string might feel most natural vertically, while a pulling could be horizontal. In close proximity pushing against the string, I modelled the force response similar to a spring, but past a threshold distance away from the string center (or "breaking point") I removed the force suddenly, thus creating a plucking sensation. I varied this threshold distance as a function of velocity, the faster (or harder) the user plucked, the smaller the distance. I felt this corresponded to the actual variation of plucking depending on the speed or force (I used velocity as a proxy for force) the user plucks the string.<br/><br/>I found one very important aspect of making the string plucking feel realistic was adding a subtle vibration force (as a function of time in a sine function) at the moment after the pluck, which decayed to 0 in less than a second. This was a very small, almost imperceptible addition that I felt made a huge difference in terms of realism for the haptics (feeling a "buzz") and visually (seeing the string oscillate). <span></span> </r-cell> <r-cell span=3> <video src="https://user-images.githubusercontent.com/17170744/160159168-f8b4a911-5cc1-4589-9238-7d1c6f6e88ce.mp4" width="100%" controls controlslist="nodownload noremoteplayback"></video> </r-cell> <r-cell span=2> In the left video, I combine both Haplys, the left Haply is again pitch control but with the circular interface created by the other teammate. The right uses the same plucking code from above, but without the visual. I also connected the string plucking and its resulting vibration to the volume of the string which produced a sound like a piano key. I found this experience to be enjoyable and easier to play that our previous iteration, in terms of haptics and sound. The sound felt less artificial (less like a pure sine wave) because of the subtle vibrato of the string. <span></span> </r-cell> <r-cell span=3> </r-cell> <r-cell span=2> <h2>Reflection</h2>From this iteration, I learned a lot about haptic rendering in terms of experience and how to structure our code for collaboration. Adding a small vibration after the plucking sensation greatly improved the realism of the overall haptic experience. I learned the key to haptic rendering is about subtlety and that users are very sensitive to small parameter change in force rendering. When recreating some haptic experience, I should focus on the details and go beyond the obvious aspects of the feeling. I draw a similar comparison when creating visuals or sound, what someone consciously notices is only a fraction of what is actually happening and the subtle additions the user may not be directly aware of greatly inform their experience nonetheless. One example could be in a song, the user may not be aware of the bass or background ambient notes, but it is essential to the feeling of the song and they would notice right away if you took it away (notice its absence but not its presence). For code collaboration, I learned how making restructures in the code for it to be more modular and have less individual parts rely on each other, helps down the road when merging other's code. I think we made a good choice to assigning each teammate's work specific for features of one Haply and then combine it in the end, because otherwise there would be too much overlap, conflict, and duplicated effort in our code. <span></span> </r-cell> <r-cell span=3> </r-cell> <r-cell span=2> <h2>Next Iteration</h2>The goal for my next iteration is to work on the sound aspect and merge my string haptics with the other teammates bow haptics. For sound, I plan to incorporate timbre and explore other variations with relating the Haply to the sound. Our idea for merging the right Haply haptics is to have a single string, with the plucking sensation at the top half and the bow sensation at the bottom. This would correspond to discrete and continuos volume control respectively. I will also focus more on the holistic playing experience, or how the device looks, feels, and sounds as both Haplys being one unified instrument. <span></span> </r-cell> </r-grid> </article> </div> <script> document.addEventListener("DOMContentLoaded", function(event) { var element = document.getElementById("-nav"); element.classList.add("active"); }); var videos = document.querySelectorAll('video[controls]'); videos.forEach(video1 => { video1.addEventListener('play', () => { videos.forEach(video2 => { if (video1 != video2) { video2.pause(); } }); }); }); </script> <section > <p class="subtitle">© Derrek Chow 2022</p> </section> </r-cell> <r-cell span=1> <nav> <a href="/" onClick="if (history.length > 1) { event.preventDefault(); history.back(); }"><b>Derrek Chow</b></a> <br/> <a class="internal" id="research-nav" href="/#research">Research</a> <a class="internal" id="design-nav" href="/#design">Design</a> <a class="internal" id="experiments-nav" href="/#experiments">Experiments</a> <a class="internal" id="art-nav" href="/#art">Art</a> <a class="internal" id="about-nav" href="/#about">About</a> <!-- <a class="internal" id="about-nav" href="/#about">About</a> --> <hr/> <a class="internal"><b>Iteration 2</b></a> </nav></r-cell> </r-grid> </main> </body> </html>